{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNS_TASK_3_AASHAY_GONDALIA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4eSNDCDYQAs"
      },
      "source": [
        "Fall Student Projects 2021: Skills Tests\n",
        "---\n",
        " Task 3 -> Retrieve CellxGene Data\n",
        "\n",
        "---\n",
        "by Aashay Gondalia (aagond@iu.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3v-c1B8hPHU"
      },
      "source": [
        "\n",
        "In this workbook, I have implemented a data fetching function that can scrap the cellxgene website data, download\n",
        "and read the datasets from all the different collections in the required format mentioned in the [google doc](https://docs.google.com/document/d/1YncjOGbgKKRJw2M0fPt5bZqdCgm9EbwyTR9DoJmZtBs/edit#). \n",
        "\n",
        "A table is prepared after inputting the required information from all the incoming data. \n",
        "\n",
        "For future work, this task can be parallelized and the RAM usage can be maintained using lazy reads ([Dask](https://dask.org/)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJGfYpoPfxOD"
      },
      "source": [
        "Installing required packages\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " -> Scanpy\n",
        "\n",
        " Scanpy is used in this workbook to read the h5 format datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8CtozhNglNs",
        "outputId": "a24a14bb-f994-46c1-9481-8334de03031e"
      },
      "source": [
        "!pip install scanpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.8.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.19.5)\n",
            "Collecting sinfo\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.1)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.1)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n",
            "Collecting anndata>=0.7.4\n",
            "  Downloading anndata-0.7.6-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.6.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.62.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.1)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.2)\n",
            "Requirement already satisfied: xlrd<2.0 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.1.2->scanpy) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->scanpy) (2018.9)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.4.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.7 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy) (2.7.3)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sinfo\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76564 sha256=b263f03881251944c552fdb8260b85f3b29ea0fe84fc58db0a072696048489c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.4-py3-none-any.whl size=52373 sha256=6f4672b3e3b2f6fa34e3e9b2dcffffb8bb58a1b234a8cd00f5dd1ec36796ff40\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/5b/62/3401692ddad12324249c774c4b15ccb046946021e2b581c043\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7899 sha256=61419a9990f2a848edefd03e3c1001d9675b8ef64fddd3e13074ac9bbfb64053\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ca/56/344d532fe53e855ccd6549795d370588ab8123907eecf4cf30\n",
            "Successfully built umap-learn pynndescent sinfo\n",
            "Installing collected packages: stdlib-list, pynndescent, umap-learn, sinfo, anndata, scanpy\n",
            "Successfully installed anndata-0.7.6 pynndescent-0.5.4 scanpy-1.8.1 sinfo-0.3.4 stdlib-list-0.8.0 umap-learn-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZECUYQTfX79o"
      },
      "source": [
        "1. Importing necessary packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EIOBswzYC_d"
      },
      "source": [
        "import datetime\n",
        "\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scanpy as sc\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p4zlWf3YkyW"
      },
      "source": [
        "2. Function to Fetch Collection Data from the https://cellxgene.cziscience.com/ website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTAQNyLmZQYb"
      },
      "source": [
        "def fetchCollectionData():\n",
        "  ## HTTP Adapter Setup\n",
        "  adapter = HTTPAdapter(max_retries=3)  #Hard-coded 3 Max Retries\n",
        "  https = requests.Session()\n",
        "  https.mount(\"https://\", adapter)\n",
        "\n",
        "  ## URL Elements\n",
        "  CELLXGENE_PRODUCTION_ENDPOINT = 'https://api.cellxgene.cziscience.com'\n",
        "  COLLECTIONS = CELLXGENE_PRODUCTION_ENDPOINT + \"/dp/v1/collections/\"\n",
        "  DATASETS = CELLXGENE_PRODUCTION_ENDPOINT + \"/dp/v1/datasets/\"\n",
        "\n",
        "  ## Fetch collection data\n",
        "  r = https.get(COLLECTIONS)\n",
        "  r.raise_for_status()\n",
        "\n",
        "  collections = sorted(r.json()['collections'], key= lambda key :key['created_at'], reverse=True)\n",
        "  print('Collection Fetch Complete.')\n",
        "  return collections, https, CELLXGENE_PRODUCTION_ENDPOINT, COLLECTIONS, DATASETS"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWRz-LmZe2-"
      },
      "source": [
        "3. Function to filter dataset - Applied Filters : {'Disease': 'Normal', 'Species': 'Homo Sapiens'}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGIQ6EHyZfTo"
      },
      "source": [
        "def filter_Dataset_Homo_Sapien_Normal(all_collections):\n",
        "  only_normal_homo_sapiens_ids = []\n",
        "  for metadata in all_collections:\n",
        "    collection_cell_counter = 0\n",
        "    for dataset in metadata['datasets']:\n",
        "      diseases = dataset['disease']\n",
        "      id = dataset['id']\n",
        "      for disease in diseases:\n",
        "        if (str(disease['label']).lower() == 'normal' and str(dataset['organism']['label']).lower() == 'homo sapiens'):\n",
        "          #Disease = disease['label']\n",
        "          #Assay = dataset['assay']\n",
        "          #Tissue = dataset['tissue']\n",
        "          #Dataset_Name = dataset['name']\n",
        "          try:\n",
        "            collection_cell_counter += dataset['cell_count']\n",
        "          except:\n",
        "            pass\n",
        "          only_normal_homo_sapiens_ids.append(id)\n",
        "  print('Dataset Filters Applied.')\n",
        "  return only_normal_homo_sapiens_ids"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_KoAaLyap3g"
      },
      "source": [
        "4. Initializing the output table. As mentioned in the google document, the dataframe column names are set accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzhheyCkapOS"
      },
      "source": [
        "def initializeTable():\n",
        "  table = pd.DataFrame({\n",
        "    'Organ/Tissue Type' : [], \n",
        "    'Cell Type CL ID' : [],\n",
        "    'HGNC/ENSEMBL Gene IDs' : [],\n",
        "    'No. of Cells of this type' : [],\n",
        "    'Disease' : [],\n",
        "    'Assay' : [],\n",
        "    'Tissue' : [],\n",
        "    'Dataset Name' : [],\n",
        "    })\n",
        "  print('Table Initialization Complete.')\n",
        "  return table\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORpS6AOSa-iM"
      },
      "source": [
        "This function is used to read the downloaded data from the cellxgene website, fetch the required information and enter it into the table. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxM9Wloya8ay"
      },
      "source": [
        "def enter_Details_into_Table(download_name, Disease, Assay, Tissue, Dataset_Name):\n",
        "  '''try:\n",
        "    table = pd.read_csv('dataTable.csv', sep='|')\n",
        "    print('Table already exists -> Imported Data')\n",
        "  except:'''\n",
        "  table = initializeTable()\n",
        "\n",
        "  print('Adding data to Table....')\n",
        "  dataset = sc.read_h5ad(download_name)\n",
        "  print('Dataset Imported in Scanpy Successfully.')\n",
        "  os.remove(download_name)\n",
        "  print('Removed dataset to aid program execution.')\n",
        "\n",
        "  #print('Dataset Reading Complete')\n",
        "  # 'Organ/Tissue Type', 'Cell Type CL ID', 'HGNC/ENSEMBL Gene IDs',\n",
        "  # 'Cells of this type', 'Disease', 'Assay', 'Tissue', 'Dataset Name'\n",
        "  \n",
        "  # Gene IDs Aggregation into a single field.\n",
        "  list_of_Genes = ''\n",
        "  for i in range(dataset.shape[1]):\n",
        "    list_of_Genes = list_of_Genes + dataset.var_names[i] + ';'\n",
        "\n",
        "  initial_dataset_no_of_rows = dataset.shape[0]\n",
        "  counter = 0\n",
        "\n",
        "  # Table data entry loop.\n",
        "  for i in range(dataset.shape[0]):\n",
        "    no_of_cells_of_same_type = int(dataset.obs.cell_type.value_counts()[dataset.obs['cell_type'][i]])\n",
        "    table.loc[len(table.index)] = [\n",
        "                              dataset.obs['tissue'][i],\n",
        "                              dataset.obs['cell_type_ontology_term_id'][i],\n",
        "                              list_of_Genes,\n",
        "                              no_of_cells_of_same_type,\n",
        "                              Disease, \n",
        "                              dataset.obs['assay'][i], \n",
        "                              Tissue, \n",
        "                              Dataset_Name\n",
        "      ]\n",
        "    counter += 1 \n",
        "  print('Data successfully added to the Table.', '\\n\\t-> Added ', counter, ' rows to the table.')\n",
        "  print(table)\n",
        "\n",
        "  return table"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeVwgDwkcQZV"
      },
      "source": [
        "Master Function is the main executable function. It calls all the above mentioned functions and saves the table in the required 'pipe-seperated' values format. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HN9Oapna5Yn"
      },
      "source": [
        "table_dataholder = None\n",
        "def masterFunction():\n",
        "  \n",
        "  collections, https, CELLXGENE_PRODUCTION_ENDPOINT, COLLECTIONS, DATASETS = fetchCollectionData()\n",
        "  all_collections = []\n",
        "\n",
        "  ## INITIAL METADATA FETCH\n",
        "  for collection in collections:\n",
        "    r1 = https.get(COLLECTIONS + collection['id'], timeout=5)\n",
        "    collection_metadata = r1.json()\n",
        "    all_collections.append(collection_metadata)\n",
        "  \n",
        "  ## Populating only_normal_homo_sapiens_ids list with all the filtered dataset ids.\n",
        "  only_normal_homo_sapiens_ids = filter_Dataset_Homo_Sapien_Normal(all_collections)\n",
        "          \n",
        "  \n",
        "  for collection in all_collections:\n",
        "    for dataset in collection['datasets']:\n",
        "      for asset in dataset['dataset_assets']:\n",
        "\n",
        "        # Using the H5 format for less overload and compatibility\n",
        "        # Faced some issues with the RDS formatting. \n",
        "        #High overload on the python wrapper to read RDS files.\n",
        "        if ((asset['filetype'] == 'H5AD') and (asset['dataset_id'] in only_normal_homo_sapiens_ids)):\n",
        "          DATASET_REQUEST = DATASETS + asset['dataset_id']  +\"/asset/\"+  asset['id']\n",
        "          \n",
        "          r2 = requests.post(DATASET_REQUEST)\n",
        "          r2.raise_for_status()\n",
        "          presigned_url = r2.json()['presigned_url']\n",
        "          \n",
        "          headers = {'range': 'bytes=0-0'}\n",
        "          r3 = https.get(presigned_url, headers=headers)\n",
        "          print('\\nDataset -> ', dataset['name'], '\\nURL -> ', presigned_url)\n",
        "          \n",
        "          if (r3.status_code == requests.codes.partial):\n",
        "            download_name = dataset['name'] + '.h5ad'\n",
        "            print('Dataset Download Started.')\n",
        "            r3 = https.get(presigned_url, timeout=10)\n",
        "            r3.raise_for_status()\n",
        "            open(download_name, 'wb').write(r3.content)\n",
        "            print('Dataset Download Complete.')\n",
        "            table = enter_Details_into_Table(download_name, dataset['disease'], dataset['assay'], dataset['tissue'], dataset['name'])\n",
        "\n",
        "            ## SAVE TABLE \n",
        "            print('Saving Table....')\n",
        "            os.mkdir(collection['name'])\n",
        "            filepath = collection['name'] + '/' + dataset['name']\n",
        "            table.to_csv(filepath, sep='|')\n",
        "            print('Table Saved Successfully.')\n",
        "            table_dataholder = table\n",
        "\n",
        "            ## To effectively use the Google COLAB RAM. \n",
        "            table = None\n",
        "            del table\n",
        "            print('Local Copy of table removed from RAM')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma6OQrfYjKm1"
      },
      "source": [
        "Master Function currently fetches collection data and downloads the dataset in a serial manner, which can be parallelized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3odARkVijKLE",
        "outputId": "d00579f6-ffc3-41ad-9695-c630987e2b3d"
      },
      "source": [
        "masterFunction()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collection Fetch Complete.\n",
            "Dataset Filters Applied.\n",
            "\n",
            "Dataset ->  Tabula Sapiens - Endothelial \n",
            "URL ->  https://corpora-data-prod.s3.amazonaws.com/5a11f879-d1ef-458a-910c-9b0bdfca5ebf/local.h5ad?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIATLYQ5N5XVFB544VL%2F20210822%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210822T174004Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIQCXNXTLtEsI0%2Btv%2Bgespit%2FxpPLYvZRTY8NPqGEzB21JAIgJOu%2FBL%2BkmnuYvyt2RQdpQRwjzUrQrQ2tV8inVuZOmGMq9AMI8v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARABGgwyMzE0MjY4NDY1NzUiDFZcApvQUWK2oeGUqCrIA19Pp%2BbD60o9JtrOQgxMisoO8me%2BSFXOtdSZWtdCi01bZGxY9heHatuQ1MTDOmAQrVXFouG%2F2K8xqKVE0y6loxom7NeAZ49mQFwZTvD8kvfG9T9qKfJqWFow%2BINdLegJcKQ4gaRBWpgSoBeaq8lmW0GLo%2BPrCBIfaW82GgtI0Jl%2Fw1jnnGa6hlt7ANGL7IY%2Fk4g2uJQpWRVF12NbiC2D8xoOk5TNm54ytzHbmL6JszanrlndLxeJgtoKjR3PCv7cFOn1euIY1pQPN8o1NXFbvo5uN3QDhPeY388%2Bb3RVjHGhQrCaAke%2FSs9SMOS0e9S%2F7lJUmERB6sdko6er3A8j0%2BTrkVW%2B1hC5KdqfTnbrEposHdI5Mw6MoNeV61eLXbwo%2BYFQiL9%2FL86cJWBMVLEmI6%2FXtXKTVm8DqPmYTsZmmz9WcUaFuAAQE1y4EJX0d8JuX7jTboXk0D8A14uj4Neiv1Q123RMET%2Faxd4sp7NucJAUcwY%2FME4G5wGJhaIHek1iapKyHg9x8oiqD5LdCXxhSFZhK3lJRsInOUJJmEnB6vdN8n5%2FBQdSH2h%2FoM2unfls5XA1EGUwmBqQGbimtD5u781vss5Vd0FnlzDuhIqJBjqlAdykJDJyGJ5IL%2Bo8VMGoq0BcK8IANOyQnKEDQ5Esn4OM%2BQirLQGFNHSgH03slnzv7ULTsJWYOSNiC9l%2BLZVS3zBB0auhBIYvKnok0LgRxbZMFnVMQqyiiskqDvsuBzTV1xCz1s0Krl5ZvEqiimljm0efurTmtWRXtUjOAv5vdwVhuaxI87BQh9LAbE9qW9wtuNWju6sJT1RDD9MIz7tXqUE8h39OQg%3D%3D&X-Amz-Signature=5cafc7e8206f473c4e257edfd2d3a43ed9c166f295fdc420749a612c0230d84a\n",
            "Dataset Download Started.\n",
            "Dataset Download Complete.\n",
            "Table Initialization Complete.\n",
            "Adding data to Table....\n",
            "Dataset Imported in Scanpy Successfully.\n",
            "Removed dataset to aid program execution.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data successfully added to the Table. \n",
            "\t-> Added  32701  rows to the table.\n",
            "      Organ/Tissue Type  ...                  Dataset Name\n",
            "0                 liver  ...  Tabula Sapiens - Endothelial\n",
            "1                 liver  ...  Tabula Sapiens - Endothelial\n",
            "2                 liver  ...  Tabula Sapiens - Endothelial\n",
            "3                 liver  ...  Tabula Sapiens - Endothelial\n",
            "4                 liver  ...  Tabula Sapiens - Endothelial\n",
            "...                 ...  ...                           ...\n",
            "32696       vasculature  ...  Tabula Sapiens - Endothelial\n",
            "32697       vasculature  ...  Tabula Sapiens - Endothelial\n",
            "32698       vasculature  ...  Tabula Sapiens - Endothelial\n",
            "32699       vasculature  ...  Tabula Sapiens - Endothelial\n",
            "32700       vasculature  ...  Tabula Sapiens - Endothelial\n",
            "\n",
            "[32701 rows x 8 columns]\n",
            "Saving Table....\n",
            "Table Saved Successfully.\n",
            "Local Copy of table removed from RAM\n",
            "\n",
            "Dataset ->  Tabula Sapiens - Immune \n",
            "URL ->  https://corpora-data-prod.s3.amazonaws.com/c5d88abe-f23a-45fa-a534-788985e93dad/local.h5ad?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIATLYQ5N5X6ESJ7LY7%2F20210822%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210822T175319Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJHMEUCIGJIbhgtruhWPvrT2cDttpXTJg3WB55x%2FeX85b1BO4poAiEAj68SR0eXaVr4WifwvoO72E48pBZ4tiI2UVBZdxoQNNEq9AMI8f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARABGgwyMzE0MjY4NDY1NzUiDPCWIKxFTn9IHSsQeyrIAwx56ANpWJV6fnJA0zJBAPJsnN%2BIe1gCKk0TuCl%2FWqTuLvEzaTBFGtyiJ3JKeqjXWQmWzTSTsQh1RMHVTfD3gDRfbPUvqgLaFT0ZBKUdHyveYteAL80KOLSMcSWoFXf5mXu5KQqsa%2FdTK%2BJlMXeqfgSNhc7Ka5GOdAr3kzk0aMmfc0LnYjXcCfUR%2BiCK%2FgBfGAp9X8WNAtzNufAr%2Blp3ULZD%2Ftp48ACD%2BvFMGandbPd0P4lwGuzDyVcNAAAZdtc5cRylb9yNo1H0FhcX5S4cokccUusuhJCNiDQZ%2FVapQCc4zK34E0AEQP5fTtJlhkgM6x37v6X4w0WP%2B4c7DxtLTJfng59j760Eu37GlTgUiDCAuYIRRT52p9EZOlX2xkgZn%2FTDstr%2FCiE%2BUQAzP0WwOKF%2FyfZdhDMspf0M2ZQHuNS3jqKieHVlAejAZjFtkrK6AtMw6T2fINeCQUAUrwpLd3VAuIcxQY%2BOd2IFRdRR6bwOsf86%2B9xSuxb7cGlXqtZYrzF2bwxvcYcFLUOzfXGM7sUcA1RIvHMb7YPA1JepkGR%2FZSD2UC5KSGy6woBxVIu%2Fg7uOnBczLu%2BZPZkIcuUiZj%2BHZMa%2B7rlJljCt8omJBjqlASsFc9H80glcBibngiLrxRrrKU0rLBkZDuJnmPX4woNGUkCwTBQE9heMU6%2B%2F%2FzJz6%2BLQ2M4cGPtMIZAQ0J6rZtOzVP6xYbz4hRFViG2tWHHL9buyQCyMZWQFHekIgULpQE%2BAEsZ9gN2UxzIhk4qpSaw%2FZj%2FiSozpsYZ3XPqgfU5bX4vW10%2FAli9XvGtuyQSjq%2FgCXC11BP93nP8PiSJ3noS%2F%2BFpWpQ%3D%3D&X-Amz-Signature=87bf0adfb0ecabe6dc8ba57c76ef1eb367710a2d788e0b0e6d5e5601fafe055b\n",
            "Dataset Download Started.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0A7t8fDEnlj"
      },
      "source": [
        "table_dataholder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNPlExUGEpGI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}